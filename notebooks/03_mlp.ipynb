{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a4ce64-f067-433a-a5be-26a4d82abaff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multilayer Perceptron (MLP) for generating Onion-like News Headlines\n",
    "\n",
    "Based on Andrej Karpathy's Youtube lecture [Building makemore Part 2: MLP](https://www.youtube.com/watch?v=TCH_1BHY58I) and Bengio et al's paper [A Neural Probabilistic Langue Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1f96d-f7a6-41b2-9c3c-67b39a3bf2a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1145c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T19:55:19.912444Z",
     "start_time": "2022-09-29T19:55:19.143987Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import pdb, sys, warnings, os, json, torch, re\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc4b39f-c56a-4d74-b633-9d067178c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_df = pd.read_csv('../data/cleaned_onion_headlines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2544e-98da-4ac9-bb15-46a5248ad5e3",
   "metadata": {},
   "source": [
    "## Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e330ce8-e357-49bc-8b64-4d019eb9d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = onion_df['text'].tolist()\n",
    "vocab = ['#'] + sorted(list(set(' '.join(texts))))\n",
    "stoi = {s:i for i,s in enumerate(vocab)}\n",
    "itos = {i:s for i,s in enumerate(vocab)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d95d1-17ab-49af-8aeb-3579cc0829ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs,ys = [],[]\n",
    "for text in texts[:1]:\n",
    "  chs = ['#'] + list(text) + ['#']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    idx1 = stoi[ch1]\n",
    "    idx2 = stoi[ch2]\n",
    "    print(ch1, ch2)\n",
    "    xs.append(idx1)\n",
    "    ys.append(idx2)\n",
    "\n",
    "xs,ys = torch.tensor(xs),torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64217acb-152e-446f-9262-0928513041da",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc396f75-f88a-4f6a-8c2b-c246fa2ec93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14922bb4-6bb2-4c2e-a209-a949848177ce",
   "metadata": {},
   "source": [
    "Make sure to cast the encoding to `float` because we don't want to pass `int` into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b80485-5161-475e-9650-e8a7b4bdadb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=55).float()\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd87a4-2531-472c-ac73-4e0d9eff2277",
   "metadata": {},
   "source": [
    "We interpret that the NN outputs `logcounts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d47bb-b786-4992-b8f9-442673f13faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d4a30-5b23-465b-896a-1f115af8c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2468975301)\n",
    "W = torch.randn((len(vocab), len(vocab)), generator=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b90cda-f1b4-4bb7-a254-e0f918aee630",
   "metadata": {},
   "source": [
    "Lines 2-3 is basically `softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64755a0c-5c84-4304-8733-edb8fdf3ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=len(vocab)).float() # input to the network: one-hot encoding\n",
    "logits = (xenc @ W)\n",
    "counts = logits.exp() # equivalent to bigram_counts\n",
    "probs = counts/counts.sum(axis=1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dbedb4-5446-475e-b311-4a243bad0386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlls = torch.zeros(len(xenc))\n",
    "for i in range(len(xenc)):\n",
    "  x = xs[i].item() # input character idx\n",
    "  y = ys[i].item() # label character idx  \n",
    "  print(f\"bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x}, {y})\")\n",
    "  print(f\"input to the NN: {x}\")\n",
    "  print(f\"output probabilities from NN: {probs[i]}\")\n",
    "  print(f\"label (actual next character): {y}\")\n",
    "  p = probs[i, y]\n",
    "  print(f\"probability assigned by the NN to the correct character: {p.item()}\")\n",
    "  logp = torch.log(p)\n",
    "  print(f\"log liklihood: {logp.item()}\")\n",
    "  nll = -logp\n",
    "  print(f\"negative log liklihood: {nll.item()}\")\n",
    "  nlls[i] = nll\n",
    "  print(\"-\"*50)\n",
    "\n",
    "print(\"=\"*50)  \n",
    "print(f\"average nll: {nlls.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3f7aa-637f-4e9d-8c9d-a83a809667af",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38586c-35ba-4c98-8ca1-8d0268d9916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb005121-1df7-4f81-9f83-407adf9cf370",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2468975301)\n",
    "W = torch.randn((len(vocab), len(vocab)), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab09371-9cbd-48c1-b70f-8bf88498fe40",
   "metadata": {},
   "source": [
    "Pluck out the probs corresponding to the indices in `ys`\n",
    "\n",
    "This is the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659b768-7bc6-4132-884b-4a76b0ce8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xenc = F.one_hot(xs, num_classes=len(vocab)).float() # input to the network: one-hot encoding\n",
    "# logits = (xenc @ W)\n",
    "logits = W[xs]\n",
    "\n",
    "# counts = logits.exp() # equivalent to bigram_counts\n",
    "# probs = counts/counts.sum(axis=1, keepdims=True)\n",
    "# loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "\n",
    "loss = F.cross_entropy(logits, ys)\n",
    "print(f\"Loss: {loss.item():0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818e9f7-cb98-4998-bda2-84956f7c98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "W.grad = None # set grad to zero\n",
    "loss.backward()\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1071c10-0616-459c-819a-3672fddeb0aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e5e20-bf1c-43e8-a3bf-8fb82a06e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_gpu = True\n",
    "device = 'cuda' if on_gpu else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab3554-8b43-4e56-831b-23027f2f713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "xs,ys = [],[]\n",
    "for text in texts:\n",
    "  chs = ['#'] + list(text) + ['#']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    idx1 = stoi[ch1]\n",
    "    idx2 = stoi[ch2]\n",
    "    xs.append(idx1)\n",
    "    ys.append(idx2)\n",
    "\n",
    "xs,ys = torch.tensor(xs, device=device),torch.tensor(ys, device=device)\n",
    "num = xs.nelement()\n",
    "print(f\"Number of examples: {num}\")\n",
    "\n",
    "# initialize the NN\n",
    "g = torch.Generator(device=device).manual_seed(2468975301)\n",
    "W = torch.randn((len(vocab), len(vocab)), generator=g, requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30735fc3-1ae5-47c5-a198-b84b27cd6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_epochs = 250\n",
    "# gradient descent\n",
    "for _ in range(n_epochs):\n",
    "  # forward pass\n",
    "  logits = W[xs]\n",
    "  loss = F.cross_entropy(logits, ys)\n",
    "  # loss = -F.cross_entropy(logits, ys) + 0.01 * (W**2).mean()\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set grad to zero\n",
    "  loss.backward()\n",
    "  # print(f\"Loss: {loss.item():0.4f}\")  \n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad\n",
    "  \n",
    "print(f\"Final Loss: {loss.item():0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a717da-6876-466e-b6ae-8795f1ef1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc87f88-2fa1-4f57-bb49-148652f079e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2468975301)\n",
    "n_headlines = 20\n",
    "for _ in range(n_headlines):\n",
    "  idx = 0\n",
    "  ai_onion = []\n",
    "  while True:    \n",
    "    # xenc = F.one_hot(torch.tensor([idx]), num_classes=len(vocab)).float()\n",
    "    # logits = xenc @ W\n",
    "    logits = W[idx].unsqueeze(0)\n",
    "    p = logits.exp() / logits.exp().sum(1, keepdims=True)\n",
    "    idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    # idx = torch.multinomial(torch.ones(len(vocab))/len(vocab), num_samples=1, replacement=True, generator=g).item()\n",
    "    ai_onion.append(itos[idx])\n",
    "    if idx == 0:\n",
    "      break\n",
    "  print(''.join(ai_onion[:-1]))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
