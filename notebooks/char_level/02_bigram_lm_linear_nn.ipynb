{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a4ce64-f067-433a-a5be-26a4d82abaff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bigram Language Modeling using a basic Neural Network for generating Onion-like News Headlines\n",
    "\n",
    "Based on Andrej Karpathy's second half Youtube lecture [The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1f96d-f7a6-41b2-9c3c-67b39a3bf2a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1145c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T19:55:19.912444Z",
     "start_time": "2022-09-29T19:55:19.143987Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import pdb, sys, warnings, os, json, torch, re\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc4b39f-c56a-4d74-b633-9d067178c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_df = pd.read_csv('../data/cleaned_onion_headlines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2544e-98da-4ac9-bb15-46a5248ad5e3",
   "metadata": {},
   "source": [
    "## Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e330ce8-e357-49bc-8b64-4d019eb9d2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = onion_df['text'].tolist()\n",
    "vocab = ['#'] + sorted(list(set(' '.join(texts))))\n",
    "stoi = {s:i for i,s in enumerate(vocab)}\n",
    "itos = {i:s for i,s in enumerate(vocab)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6d95d1-17ab-49af-8aeb-3579cc0829ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# e\n",
      "e n\n",
      "n t\n",
      "t i\n",
      "i r\n",
      "r e\n",
      "e  \n",
      "  f\n",
      "f a\n",
      "a c\n",
      "c e\n",
      "e b\n",
      "b o\n",
      "o o\n",
      "o k\n",
      "k  \n",
      "  s\n",
      "s t\n",
      "t a\n",
      "a f\n",
      "f f\n",
      "f  \n",
      "  l\n",
      "l a\n",
      "a u\n",
      "u g\n",
      "g h\n",
      "h s\n",
      "s  \n",
      "  a\n",
      "a s\n",
      "s  \n",
      "  m\n",
      "m a\n",
      "a n\n",
      "n  \n",
      "  t\n",
      "t i\n",
      "i g\n",
      "g h\n",
      "h t\n",
      "t e\n",
      "e n\n",
      "n s\n",
      "s  \n",
      "  p\n",
      "p r\n",
      "r i\n",
      "i v\n",
      "v a\n",
      "a c\n",
      "c y\n",
      "y  \n",
      "  s\n",
      "s e\n",
      "e t\n",
      "t t\n",
      "t i\n",
      "i n\n",
      "n g\n",
      "g s\n",
      "s #\n"
     ]
    }
   ],
   "source": [
    "xs,ys = [],[]\n",
    "for text in texts[:1]:\n",
    "  chs = ['#'] + list(text) + ['#']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    idx1 = stoi[ch1]\n",
    "    idx2 = stoi[ch2]\n",
    "    print(ch1, ch2)\n",
    "    xs.append(idx1)\n",
    "    ys.append(idx2)\n",
    "\n",
    "xs,ys = torch.tensor(xs),torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64217acb-152e-446f-9262-0928513041da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  6, 15, 21, 10, 19,  6,  1,  7,  2,  4,  6,  3, 16, 16, 12,  1, 20,\n",
       "        21,  2,  7,  7,  1, 13,  2, 22,  8,  9, 20,  1,  2, 20,  1, 14,  2, 15,\n",
       "         1, 21, 10,  8,  9, 21,  6, 15, 20,  1, 17, 19, 10, 23,  2,  4, 26,  1,\n",
       "        20,  6, 21, 21, 10, 15,  8, 20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc396f75-f88a-4f6a-8c2b-c246fa2ec93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 15, 21, 10, 19,  6,  1,  7,  2,  4,  6,  3, 16, 16, 12,  1, 20, 21,\n",
       "         2,  7,  7,  1, 13,  2, 22,  8,  9, 20,  1,  2, 20,  1, 14,  2, 15,  1,\n",
       "        21, 10,  8,  9, 21,  6, 15, 20,  1, 17, 19, 10, 23,  2,  4, 26,  1, 20,\n",
       "         6, 21, 21, 10, 15,  8, 20,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14922bb4-6bb2-4c2e-a209-a949848177ce",
   "metadata": {},
   "source": [
    "Make sure to cast the encoding to `float` because we don't want to pass `int` into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b80485-5161-475e-9650-e8a7b4bdadb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f56b9d2ae50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=55).float()\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd87a4-2531-472c-ac73-4e0d9eff2277",
   "metadata": {},
   "source": [
    "We interpret that the NN outputs `logcounts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97d47bb-b786-4992-b8f9-442673f13faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62, 55])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "737d4a30-5b23-465b-896a-1f115af8c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2468975301)\n",
    "W = torch.randn((len(vocab), len(vocab)), generator=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b90cda-f1b4-4bb7-a254-e0f918aee630",
   "metadata": {},
   "source": [
    "Lines 2-3 is basically `softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64755a0c-5c84-4304-8733-edb8fdf3ea4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0525, 0.0682, 0.0373,  ..., 0.0470, 0.0440, 0.0491],\n",
       "        [0.0255, 0.0291, 0.0103,  ..., 0.0218, 0.0144, 0.0379],\n",
       "        [0.0032, 0.0158, 0.0135,  ..., 0.0158, 0.0030, 0.0647],\n",
       "        ...,\n",
       "        [0.0032, 0.0158, 0.0135,  ..., 0.0158, 0.0030, 0.0647],\n",
       "        [0.0348, 0.0112, 0.0364,  ..., 0.0048, 0.0147, 0.0272],\n",
       "        [0.0048, 0.0077, 0.0425,  ..., 0.0393, 0.0173, 0.0207]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=len(vocab)).float() # input to the network: one-hot encoding\n",
    "logits = (xenc @ W)\n",
    "counts = logits.exp() # equivalent to bigram_counts\n",
    "probs = counts/counts.sum(axis=1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7dbedb4-5446-475e-b311-4a243bad0386",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram example 1: #e (indexes 0, 6)\n",
      "input to the NN: 0\n",
      "output probabilities from NN: tensor([0.0525, 0.0682, 0.0373, 0.0073, 0.0791, 0.0085, 0.0455, 0.0190, 0.0289,\n",
      "        0.1439, 0.0186, 0.0139, 0.1082, 0.0366, 0.0243, 0.0070, 0.0039, 0.0153,\n",
      "        0.0210, 0.0089, 0.0232, 0.0331, 0.0381, 0.0116, 0.0057, 0.0470, 0.0440,\n",
      "        0.0491])\n",
      "label (actual next character): 6\n",
      "probability assigned by the NN to the correct character: 0.04553060233592987\n",
      "log liklihood: -3.0893707275390625\n",
      "negative log liklihood: 3.0893707275390625\n",
      "--------------------------------------------------\n",
      "bigram example 2: en (indexes 6, 15)\n",
      "input to the NN: 6\n",
      "output probabilities from NN: tensor([0.0255, 0.0291, 0.0103, 0.0394, 0.0265, 0.0269, 0.0278, 0.0175, 0.0220,\n",
      "        0.0202, 0.0044, 0.0242, 0.0738, 0.0584, 0.0406, 0.0430, 0.0163, 0.1088,\n",
      "        0.0200, 0.0537, 0.0208, 0.0211, 0.0940, 0.0918, 0.0097, 0.0218, 0.0144,\n",
      "        0.0379])\n",
      "label (actual next character): 15\n",
      "probability assigned by the NN to the correct character: 0.042962007224559784\n",
      "log liklihood: -3.1474390029907227\n",
      "negative log liklihood: 3.1474390029907227\n",
      "--------------------------------------------------\n",
      "bigram example 3: nt (indexes 15, 21)\n",
      "input to the NN: 15\n",
      "output probabilities from NN: tensor([0.0032, 0.0158, 0.0135, 0.0232, 0.0397, 0.0053, 0.0093, 0.0933, 0.0463,\n",
      "        0.0188, 0.0180, 0.1058, 0.0125, 0.0503, 0.0083, 0.0349, 0.0410, 0.0474,\n",
      "        0.0386, 0.0234, 0.0234, 0.0199, 0.2096, 0.0028, 0.0124, 0.0158, 0.0030,\n",
      "        0.0647])\n",
      "label (actual next character): 21\n",
      "probability assigned by the NN to the correct character: 0.01993202604353428\n",
      "log liklihood: -3.9154274463653564\n",
      "negative log liklihood: 3.9154274463653564\n",
      "--------------------------------------------------\n",
      "bigram example 4: ti (indexes 21, 10)\n",
      "input to the NN: 21\n",
      "output probabilities from NN: tensor([0.0337, 0.0284, 0.0815, 0.0058, 0.0136, 0.1453, 0.1755, 0.0123, 0.0618,\n",
      "        0.0460, 0.0221, 0.0273, 0.0174, 0.0236, 0.0079, 0.0025, 0.0136, 0.0316,\n",
      "        0.0887, 0.0153, 0.0113, 0.0255, 0.0068, 0.0341, 0.0141, 0.0103, 0.0035,\n",
      "        0.0405])\n",
      "label (actual next character): 10\n",
      "probability assigned by the NN to the correct character: 0.022112660109996796\n",
      "log liklihood: -3.8116049766540527\n",
      "negative log liklihood: 3.8116049766540527\n",
      "--------------------------------------------------\n",
      "bigram example 5: ir (indexes 10, 19)\n",
      "input to the NN: 10\n",
      "output probabilities from NN: tensor([0.0854, 0.1028, 0.0387, 0.0191, 0.0048, 0.0318, 0.0419, 0.0060, 0.0227,\n",
      "        0.0474, 0.0150, 0.0245, 0.0299, 0.0197, 0.0073, 0.1017, 0.0457, 0.0382,\n",
      "        0.0137, 0.0026, 0.0085, 0.0797, 0.0706, 0.0187, 0.0071, 0.0528, 0.0452,\n",
      "        0.0185])\n",
      "label (actual next character): 19\n",
      "probability assigned by the NN to the correct character: 0.002636313671246171\n",
      "log liklihood: -5.938373565673828\n",
      "negative log liklihood: 5.938373565673828\n",
      "--------------------------------------------------\n",
      "bigram example 6: re (indexes 19, 6)\n",
      "input to the NN: 19\n",
      "output probabilities from NN: tensor([0.0838, 0.0088, 0.0366, 0.0497, 0.0384, 0.0081, 0.0034, 0.0290, 0.0076,\n",
      "        0.0052, 0.0016, 0.0179, 0.1613, 0.0068, 0.1656, 0.0634, 0.0736, 0.0104,\n",
      "        0.0500, 0.0170, 0.0133, 0.0475, 0.0207, 0.0060, 0.0327, 0.0068, 0.0309,\n",
      "        0.0038])\n",
      "label (actual next character): 6\n",
      "probability assigned by the NN to the correct character: 0.003430251032114029\n",
      "log liklihood: -5.675121784210205\n",
      "negative log liklihood: 5.675121784210205\n",
      "--------------------------------------------------\n",
      "bigram example 7: e  (indexes 6, 1)\n",
      "input to the NN: 6\n",
      "output probabilities from NN: tensor([0.0255, 0.0291, 0.0103, 0.0394, 0.0265, 0.0269, 0.0278, 0.0175, 0.0220,\n",
      "        0.0202, 0.0044, 0.0242, 0.0738, 0.0584, 0.0406, 0.0430, 0.0163, 0.1088,\n",
      "        0.0200, 0.0537, 0.0208, 0.0211, 0.0940, 0.0918, 0.0097, 0.0218, 0.0144,\n",
      "        0.0379])\n",
      "label (actual next character): 1\n",
      "probability assigned by the NN to the correct character: 0.029121972620487213\n",
      "log liklihood: -3.536262273788452\n",
      "negative log liklihood: 3.536262273788452\n",
      "--------------------------------------------------\n",
      "bigram example 8:  f (indexes 1, 7)\n",
      "input to the NN: 1\n",
      "output probabilities from NN: tensor([0.0119, 0.0097, 0.1096, 0.1361, 0.0127, 0.0202, 0.0118, 0.0369, 0.0302,\n",
      "        0.0489, 0.0103, 0.0309, 0.0133, 0.0475, 0.0178, 0.0232, 0.0131, 0.0185,\n",
      "        0.0291, 0.0016, 0.0130, 0.0558, 0.0148, 0.0051, 0.0508, 0.2155, 0.0048,\n",
      "        0.0069])\n",
      "label (actual next character): 7\n",
      "probability assigned by the NN to the correct character: 0.03688538819551468\n",
      "log liklihood: -3.2999398708343506\n",
      "negative log liklihood: 3.2999398708343506\n",
      "--------------------------------------------------\n",
      "bigram example 9: fa (indexes 7, 2)\n",
      "input to the NN: 7\n",
      "output probabilities from NN: tensor([0.0104, 0.0127, 0.0269, 0.0277, 0.0308, 0.0701, 0.0071, 0.0259, 0.0084,\n",
      "        0.0902, 0.0539, 0.1869, 0.0102, 0.0064, 0.0346, 0.0358, 0.0146, 0.0076,\n",
      "        0.0122, 0.0119, 0.0030, 0.0197, 0.0525, 0.0246, 0.0117, 0.0190, 0.1691,\n",
      "        0.0163])\n",
      "label (actual next character): 2\n",
      "probability assigned by the NN to the correct character: 0.02687169425189495\n",
      "log liklihood: -3.6166818141937256\n",
      "negative log liklihood: 3.6166818141937256\n",
      "--------------------------------------------------\n",
      "bigram example 10: ac (indexes 2, 4)\n",
      "input to the NN: 2\n",
      "output probabilities from NN: tensor([0.0809, 0.0298, 0.0059, 0.0271, 0.0130, 0.0065, 0.0135, 0.0184, 0.0199,\n",
      "        0.0375, 0.0255, 0.0155, 0.0187, 0.0493, 0.0560, 0.0375, 0.0613, 0.0282,\n",
      "        0.0662, 0.0124, 0.0652, 0.0301, 0.1511, 0.0179, 0.0273, 0.0105, 0.0455,\n",
      "        0.0294])\n",
      "label (actual next character): 4\n",
      "probability assigned by the NN to the correct character: 0.012994052842259407\n",
      "log liklihood: -4.343263626098633\n",
      "negative log liklihood: 4.343263626098633\n",
      "--------------------------------------------------\n",
      "bigram example 11: ce (indexes 4, 6)\n",
      "input to the NN: 4\n",
      "output probabilities from NN: tensor([0.0068, 0.0256, 0.0106, 0.0459, 0.0117, 0.1405, 0.0629, 0.0394, 0.0306,\n",
      "        0.0380, 0.0610, 0.0384, 0.0274, 0.0041, 0.0374, 0.0324, 0.0079, 0.0772,\n",
      "        0.0401, 0.0341, 0.0074, 0.0091, 0.0564, 0.0517, 0.0083, 0.0178, 0.0199,\n",
      "        0.0573])\n",
      "label (actual next character): 6\n",
      "probability assigned by the NN to the correct character: 0.06287892162799835\n",
      "log liklihood: -2.7665443420410156\n",
      "negative log liklihood: 2.7665443420410156\n",
      "--------------------------------------------------\n",
      "bigram example 12: eb (indexes 6, 3)\n",
      "input to the NN: 6\n",
      "output probabilities from NN: tensor([0.0255, 0.0291, 0.0103, 0.0394, 0.0265, 0.0269, 0.0278, 0.0175, 0.0220,\n",
      "        0.0202, 0.0044, 0.0242, 0.0738, 0.0584, 0.0406, 0.0430, 0.0163, 0.1088,\n",
      "        0.0200, 0.0537, 0.0208, 0.0211, 0.0940, 0.0918, 0.0097, 0.0218, 0.0144,\n",
      "        0.0379])\n",
      "label (actual next character): 3\n",
      "probability assigned by the NN to the correct character: 0.03943901136517525\n",
      "log liklihood: -3.232999801635742\n",
      "negative log liklihood: 3.232999801635742\n",
      "--------------------------------------------------\n",
      "bigram example 13: bo (indexes 3, 16)\n",
      "input to the NN: 3\n",
      "output probabilities from NN: tensor([0.0704, 0.1029, 0.0127, 0.0180, 0.0287, 0.1110, 0.0109, 0.0571, 0.0199,\n",
      "        0.0118, 0.0388, 0.0340, 0.0134, 0.0339, 0.0011, 0.0092, 0.0206, 0.1692,\n",
      "        0.0202, 0.0120, 0.0043, 0.0121, 0.0064, 0.0517, 0.0305, 0.0337, 0.0399,\n",
      "        0.0257])\n",
      "label (actual next character): 16\n",
      "probability assigned by the NN to the correct character: 0.020649241283535957\n",
      "log liklihood: -3.8800766468048096\n",
      "negative log liklihood: 3.8800766468048096\n",
      "--------------------------------------------------\n",
      "bigram example 14: oo (indexes 16, 16)\n",
      "input to the NN: 16\n",
      "output probabilities from NN: tensor([0.0029, 0.0202, 0.0293, 0.0200, 0.0111, 0.0257, 0.0466, 0.0445, 0.0378,\n",
      "        0.0094, 0.0047, 0.0178, 0.0250, 0.0124, 0.0305, 0.0148, 0.1267, 0.0745,\n",
      "        0.0721, 0.0329, 0.0229, 0.0063, 0.0323, 0.0226, 0.0561, 0.1351, 0.0353,\n",
      "        0.0305])\n",
      "label (actual next character): 16\n",
      "probability assigned by the NN to the correct character: 0.126718670129776\n",
      "log liklihood: -2.0657858848571777\n",
      "negative log liklihood: 2.0657858848571777\n",
      "--------------------------------------------------\n",
      "bigram example 15: ok (indexes 16, 12)\n",
      "input to the NN: 16\n",
      "output probabilities from NN: tensor([0.0029, 0.0202, 0.0293, 0.0200, 0.0111, 0.0257, 0.0466, 0.0445, 0.0378,\n",
      "        0.0094, 0.0047, 0.0178, 0.0250, 0.0124, 0.0305, 0.0148, 0.1267, 0.0745,\n",
      "        0.0721, 0.0329, 0.0229, 0.0063, 0.0323, 0.0226, 0.0561, 0.1351, 0.0353,\n",
      "        0.0305])\n",
      "label (actual next character): 12\n",
      "probability assigned by the NN to the correct character: 0.024974094703793526\n",
      "log liklihood: -3.6899161338806152\n",
      "negative log liklihood: 3.6899161338806152\n",
      "--------------------------------------------------\n",
      "bigram example 16: k  (indexes 12, 1)\n",
      "input to the NN: 12\n",
      "output probabilities from NN: tensor([0.0094, 0.0212, 0.0103, 0.0209, 0.0047, 0.0154, 0.0077, 0.0043, 0.2444,\n",
      "        0.0238, 0.1713, 0.0284, 0.0141, 0.0429, 0.0026, 0.0123, 0.0221, 0.0374,\n",
      "        0.0121, 0.0034, 0.0300, 0.0161, 0.0494, 0.1045, 0.0031, 0.0208, 0.0166,\n",
      "        0.0509])\n",
      "label (actual next character): 1\n",
      "probability assigned by the NN to the correct character: 0.02116783708333969\n",
      "log liklihood: -3.8552722930908203\n",
      "negative log liklihood: 3.8552722930908203\n",
      "--------------------------------------------------\n",
      "bigram example 17:  s (indexes 1, 20)\n",
      "input to the NN: 1\n",
      "output probabilities from NN: tensor([0.0119, 0.0097, 0.1096, 0.1361, 0.0127, 0.0202, 0.0118, 0.0369, 0.0302,\n",
      "        0.0489, 0.0103, 0.0309, 0.0133, 0.0475, 0.0178, 0.0232, 0.0131, 0.0185,\n",
      "        0.0291, 0.0016, 0.0130, 0.0558, 0.0148, 0.0051, 0.0508, 0.2155, 0.0048,\n",
      "        0.0069])\n",
      "label (actual next character): 20\n",
      "probability assigned by the NN to the correct character: 0.013045123778283596\n",
      "log liklihood: -4.339340686798096\n",
      "negative log liklihood: 4.339340686798096\n",
      "--------------------------------------------------\n",
      "bigram example 18: st (indexes 20, 21)\n",
      "input to the NN: 20\n",
      "output probabilities from NN: tensor([0.0048, 0.0077, 0.0425, 0.0983, 0.0717, 0.0138, 0.0121, 0.0158, 0.0082,\n",
      "        0.0832, 0.0275, 0.0105, 0.0226, 0.0956, 0.0882, 0.0148, 0.0405, 0.0131,\n",
      "        0.0333, 0.0110, 0.0212, 0.0104, 0.0212, 0.0426, 0.1122, 0.0393, 0.0173,\n",
      "        0.0207])\n",
      "label (actual next character): 21\n",
      "probability assigned by the NN to the correct character: 0.010429025627672672\n",
      "log liklihood: -4.563162326812744\n",
      "negative log liklihood: 4.563162326812744\n",
      "--------------------------------------------------\n",
      "bigram example 19: ta (indexes 21, 2)\n",
      "input to the NN: 21\n",
      "output probabilities from NN: tensor([0.0337, 0.0284, 0.0815, 0.0058, 0.0136, 0.1453, 0.1755, 0.0123, 0.0618,\n",
      "        0.0460, 0.0221, 0.0273, 0.0174, 0.0236, 0.0079, 0.0025, 0.0136, 0.0316,\n",
      "        0.0887, 0.0153, 0.0113, 0.0255, 0.0068, 0.0341, 0.0141, 0.0103, 0.0035,\n",
      "        0.0405])\n",
      "label (actual next character): 2\n",
      "probability assigned by the NN to the correct character: 0.08148515969514847\n",
      "log liklihood: -2.5073344707489014\n",
      "negative log liklihood: 2.5073344707489014\n",
      "--------------------------------------------------\n",
      "bigram example 20: af (indexes 2, 7)\n",
      "input to the NN: 2\n",
      "output probabilities from NN: tensor([0.0809, 0.0298, 0.0059, 0.0271, 0.0130, 0.0065, 0.0135, 0.0184, 0.0199,\n",
      "        0.0375, 0.0255, 0.0155, 0.0187, 0.0493, 0.0560, 0.0375, 0.0613, 0.0282,\n",
      "        0.0662, 0.0124, 0.0652, 0.0301, 0.1511, 0.0179, 0.0273, 0.0105, 0.0455,\n",
      "        0.0294])\n",
      "label (actual next character): 7\n",
      "probability assigned by the NN to the correct character: 0.01837260089814663\n",
      "log liklihood: -3.9968948364257812\n",
      "negative log liklihood: 3.9968948364257812\n",
      "--------------------------------------------------\n",
      "bigram example 21: ff (indexes 7, 7)\n",
      "input to the NN: 7\n",
      "output probabilities from NN: tensor([0.0104, 0.0127, 0.0269, 0.0277, 0.0308, 0.0701, 0.0071, 0.0259, 0.0084,\n",
      "        0.0902, 0.0539, 0.1869, 0.0102, 0.0064, 0.0346, 0.0358, 0.0146, 0.0076,\n",
      "        0.0122, 0.0119, 0.0030, 0.0197, 0.0525, 0.0246, 0.0117, 0.0190, 0.1691,\n",
      "        0.0163])\n",
      "label (actual next character): 7\n",
      "probability assigned by the NN to the correct character: 0.025908976793289185\n",
      "log liklihood: -3.653165817260742\n",
      "negative log liklihood: 3.653165817260742\n",
      "--------------------------------------------------\n",
      "bigram example 22: f  (indexes 7, 1)\n",
      "input to the NN: 7\n",
      "output probabilities from NN: tensor([0.0104, 0.0127, 0.0269, 0.0277, 0.0308, 0.0701, 0.0071, 0.0259, 0.0084,\n",
      "        0.0902, 0.0539, 0.1869, 0.0102, 0.0064, 0.0346, 0.0358, 0.0146, 0.0076,\n",
      "        0.0122, 0.0119, 0.0030, 0.0197, 0.0525, 0.0246, 0.0117, 0.0190, 0.1691,\n",
      "        0.0163])\n",
      "label (actual next character): 1\n",
      "probability assigned by the NN to the correct character: 0.01273216214030981\n",
      "log liklihood: -4.363624095916748\n",
      "negative log liklihood: 4.363624095916748\n",
      "--------------------------------------------------\n",
      "bigram example 23:  l (indexes 1, 13)\n",
      "input to the NN: 1\n",
      "output probabilities from NN: tensor([0.0119, 0.0097, 0.1096, 0.1361, 0.0127, 0.0202, 0.0118, 0.0369, 0.0302,\n",
      "        0.0489, 0.0103, 0.0309, 0.0133, 0.0475, 0.0178, 0.0232, 0.0131, 0.0185,\n",
      "        0.0291, 0.0016, 0.0130, 0.0558, 0.0148, 0.0051, 0.0508, 0.2155, 0.0048,\n",
      "        0.0069])\n",
      "label (actual next character): 13\n",
      "probability assigned by the NN to the correct character: 0.04750484973192215\n",
      "log liklihood: -3.0469233989715576\n",
      "negative log liklihood: 3.0469233989715576\n",
      "--------------------------------------------------\n",
      "bigram example 24: la (indexes 13, 2)\n",
      "input to the NN: 13\n",
      "output probabilities from NN: tensor([0.0373, 0.0054, 0.0600, 0.0139, 0.0381, 0.0268, 0.0474, 0.0283, 0.0135,\n",
      "        0.0135, 0.1460, 0.0428, 0.1473, 0.0149, 0.0146, 0.0194, 0.0242, 0.0135,\n",
      "        0.0073, 0.0347, 0.0394, 0.0115, 0.0495, 0.0184, 0.0167, 0.0103, 0.0622,\n",
      "        0.0430])\n",
      "label (actual next character): 2\n",
      "probability assigned by the NN to the correct character: 0.060034096240997314\n",
      "log liklihood: -2.812842607498169\n",
      "negative log liklihood: 2.812842607498169\n",
      "--------------------------------------------------\n",
      "bigram example 25: au (indexes 2, 22)\n",
      "input to the NN: 2\n",
      "output probabilities from NN: tensor([0.0809, 0.0298, 0.0059, 0.0271, 0.0130, 0.0065, 0.0135, 0.0184, 0.0199,\n",
      "        0.0375, 0.0255, 0.0155, 0.0187, 0.0493, 0.0560, 0.0375, 0.0613, 0.0282,\n",
      "        0.0662, 0.0124, 0.0652, 0.0301, 0.1511, 0.0179, 0.0273, 0.0105, 0.0455,\n",
      "        0.0294])\n",
      "label (actual next character): 22\n",
      "probability assigned by the NN to the correct character: 0.1511322408914566\n",
      "log liklihood: -1.8896000385284424\n",
      "negative log liklihood: 1.8896000385284424\n",
      "--------------------------------------------------\n",
      "bigram example 26: ug (indexes 22, 8)\n",
      "input to the NN: 22\n",
      "output probabilities from NN: tensor([0.1588, 0.0387, 0.0236, 0.1878, 0.0081, 0.0307, 0.0244, 0.0127, 0.0259,\n",
      "        0.0099, 0.0425, 0.0197, 0.0113, 0.0265, 0.0589, 0.0257, 0.0334, 0.0116,\n",
      "        0.0191, 0.0115, 0.0171, 0.0048, 0.0298, 0.0476, 0.0051, 0.0268, 0.0324,\n",
      "        0.0558])\n",
      "label (actual next character): 8\n",
      "probability assigned by the NN to the correct character: 0.02590898796916008\n",
      "log liklihood: -3.653165340423584\n",
      "negative log liklihood: 3.653165340423584\n",
      "--------------------------------------------------\n",
      "bigram example 27: gh (indexes 8, 9)\n",
      "input to the NN: 8\n",
      "output probabilities from NN: tensor([0.0348, 0.0112, 0.0364, 0.0847, 0.0041, 0.0179, 0.0126, 0.0098, 0.0270,\n",
      "        0.0902, 0.0225, 0.0170, 0.0099, 0.0126, 0.0446, 0.1308, 0.0048, 0.0281,\n",
      "        0.0286, 0.0090, 0.0931, 0.0537, 0.1149, 0.0121, 0.0428, 0.0048, 0.0147,\n",
      "        0.0272])\n",
      "label (actual next character): 9\n",
      "probability assigned by the NN to the correct character: 0.09020441025495529\n",
      "log liklihood: -2.40567684173584\n",
      "negative log liklihood: 2.40567684173584\n",
      "--------------------------------------------------\n",
      "bigram example 28: hs (indexes 9, 20)\n",
      "input to the NN: 9\n",
      "output probabilities from NN: tensor([0.0154, 0.1070, 0.0315, 0.0141, 0.0123, 0.0125, 0.0587, 0.0576, 0.0532,\n",
      "        0.0462, 0.0114, 0.0458, 0.0551, 0.0175, 0.0063, 0.0117, 0.0090, 0.0112,\n",
      "        0.0161, 0.2724, 0.0091, 0.0486, 0.0180, 0.0047, 0.0063, 0.0208, 0.0203,\n",
      "        0.0073])\n",
      "label (actual next character): 20\n",
      "probability assigned by the NN to the correct character: 0.009087934158742428\n",
      "log liklihood: -4.700807571411133\n",
      "negative log liklihood: 4.700807571411133\n",
      "--------------------------------------------------\n",
      "bigram example 29: s  (indexes 20, 1)\n",
      "input to the NN: 20\n",
      "output probabilities from NN: tensor([0.0048, 0.0077, 0.0425, 0.0983, 0.0717, 0.0138, 0.0121, 0.0158, 0.0082,\n",
      "        0.0832, 0.0275, 0.0105, 0.0226, 0.0956, 0.0882, 0.0148, 0.0405, 0.0131,\n",
      "        0.0333, 0.0110, 0.0212, 0.0104, 0.0212, 0.0426, 0.1122, 0.0393, 0.0173,\n",
      "        0.0207])\n",
      "label (actual next character): 1\n",
      "probability assigned by the NN to the correct character: 0.007655725348740816\n",
      "log liklihood: -4.8723015785217285\n",
      "negative log liklihood: 4.8723015785217285\n",
      "--------------------------------------------------\n",
      "bigram example 30:  a (indexes 1, 2)\n",
      "input to the NN: 1\n",
      "output probabilities from NN: tensor([0.0119, 0.0097, 0.1096, 0.1361, 0.0127, 0.0202, 0.0118, 0.0369, 0.0302,\n",
      "        0.0489, 0.0103, 0.0309, 0.0133, 0.0475, 0.0178, 0.0232, 0.0131, 0.0185,\n",
      "        0.0291, 0.0016, 0.0130, 0.0558, 0.0148, 0.0051, 0.0508, 0.2155, 0.0048,\n",
      "        0.0069])\n",
      "label (actual next character): 2\n",
      "probability assigned by the NN to the correct character: 0.10956306755542755\n",
      "log liklihood: -2.211254835128784\n",
      "negative log liklihood: 2.211254835128784\n",
      "--------------------------------------------------\n",
      "bigram example 31: as (indexes 2, 20)\n",
      "input to the NN: 2\n",
      "output probabilities from NN: tensor([0.0809, 0.0298, 0.0059, 0.0271, 0.0130, 0.0065, 0.0135, 0.0184, 0.0199,\n",
      "        0.0375, 0.0255, 0.0155, 0.0187, 0.0493, 0.0560, 0.0375, 0.0613, 0.0282,\n",
      "        0.0662, 0.0124, 0.0652, 0.0301, 0.1511, 0.0179, 0.0273, 0.0105, 0.0455,\n",
      "        0.0294])\n",
      "label (actual next character): 20\n",
      "probability assigned by the NN to the correct character: 0.06519952416419983\n",
      "log liklihood: -2.7303030490875244\n",
      "negative log liklihood: 2.7303030490875244\n",
      "--------------------------------------------------\n",
      "bigram example 32: s  (indexes 20, 1)\n",
      "input to the NN: 20\n",
      "output probabilities from NN: tensor([0.0048, 0.0077, 0.0425, 0.0983, 0.0717, 0.0138, 0.0121, 0.0158, 0.0082,\n",
      "        0.0832, 0.0275, 0.0105, 0.0226, 0.0956, 0.0882, 0.0148, 0.0405, 0.0131,\n",
      "        0.0333, 0.0110, 0.0212, 0.0104, 0.0212, 0.0426, 0.1122, 0.0393, 0.0173,\n",
      "        0.0207])\n",
      "label (actual next character): 1\n",
      "probability assigned by the NN to the correct character: 0.007655725348740816\n",
      "log liklihood: -4.8723015785217285\n",
      "negative log liklihood: 4.8723015785217285\n",
      "--------------------------------------------------\n",
      "bigram example 33:  m (indexes 1, 14)\n",
      "input to the NN: 1\n",
      "output probabilities from NN: tensor([0.0119, 0.0097, 0.1096, 0.1361, 0.0127, 0.0202, 0.0118, 0.0369, 0.0302,\n",
      "        0.0489, 0.0103, 0.0309, 0.0133, 0.0475, 0.0178, 0.0232, 0.0131, 0.0185,\n",
      "        0.0291, 0.0016, 0.0130, 0.0558, 0.0148, 0.0051, 0.0508, 0.2155, 0.0048,\n",
      "        0.0069])\n",
      "label (actual next character): 14\n",
      "probability assigned by the NN to the correct character: 0.017847085371613503\n",
      "log liklihood: -4.025915145874023\n",
      "negative log liklihood: 4.025915145874023\n",
      "--------------------------------------------------\n",
      "bigram example 34: ma (indexes 14, 2)\n",
      "input to the NN: 14\n",
      "output probabilities from NN: tensor([0.0370, 0.0263, 0.0127, 0.0034, 0.0206, 0.0204, 0.0247, 0.0215, 0.0422,\n",
      "        0.0445, 0.0223, 0.0839, 0.0133, 0.0724, 0.0180, 0.0144, 0.0027, 0.0551,\n",
      "        0.0209, 0.0358, 0.0142, 0.0239, 0.0395, 0.0047, 0.0479, 0.0103, 0.0553,\n",
      "        0.2121])\n",
      "label (actual next character): 2\n",
      "probability assigned by the NN to the correct character: 0.012687979266047478\n",
      "log liklihood: -4.367100238800049\n",
      "negative log liklihood: 4.367100238800049\n",
      "--------------------------------------------------\n",
      "bigram example 35: an (indexes 2, 15)\n",
      "input to the NN: 2\n",
      "output probabilities from NN: tensor([0.0809, 0.0298, 0.0059, 0.0271, 0.0130, 0.0065, 0.0135, 0.0184, 0.0199,\n",
      "        0.0375, 0.0255, 0.0155, 0.0187, 0.0493, 0.0560, 0.0375, 0.0613, 0.0282,\n",
      "        0.0662, 0.0124, 0.0652, 0.0301, 0.1511, 0.0179, 0.0273, 0.0105, 0.0455,\n",
      "        0.0294])\n",
      "label (actual next character): 15\n",
      "probability assigned by the NN to the correct character: 0.03746678680181503\n",
      "log liklihood: -3.2843003273010254\n",
      "negative log liklihood: 3.2843003273010254\n",
      "--------------------------------------------------\n",
      "bigram example 36: n  (indexes 15, 1)\n",
      "input to the NN: 15\n",
      "output probabilities from NN: tensor([0.0032, 0.0158, 0.0135, 0.0232, 0.0397, 0.0053, 0.0093, 0.0933, 0.0463,\n",
      "        0.0188, 0.0180, 0.1058, 0.0125, 0.0503, 0.0083, 0.0349, 0.0410, 0.0474,\n",
      "        0.0386, 0.0234, 0.0234, 0.0199, 0.2096, 0.0028, 0.0124, 0.0158, 0.0030,\n",
      "        0.0647])\n",
      "label (actual next character): 1\n",
      "probability assigned by the NN to the correct character: 0.015835372731089592\n",
      "log liklihood: -4.145509243011475\n",
      "negative log liklihood: 4.145509243011475\n",
      "--------------------------------------------------\n",
      "bigram example 37:  t (indexes 1, 21)\n",
      "input to the NN: 1\n",
      "output probabilities from NN: tensor([0.0119, 0.0097, 0.1096, 0.1361, 0.0127, 0.0202, 0.0118, 0.0369, 0.0302,\n",
      "        0.0489, 0.0103, 0.0309, 0.0133, 0.0475, 0.0178, 0.0232, 0.0131, 0.0185,\n",
      "        0.0291, 0.0016, 0.0130, 0.0558, 0.0148, 0.0051, 0.0508, 0.2155, 0.0048,\n",
      "        0.0069])\n",
      "label (actual next character): 21\n",
      "probability assigned by the NN to the correct character: 0.05577647686004639\n",
      "log liklihood: -2.8864030838012695\n",
      "negative log liklihood: 2.8864030838012695\n",
      "--------------------------------------------------\n",
      "bigram example 38: ti (indexes 21, 10)\n",
      "input to the NN: 21\n",
      "output probabilities from NN: tensor([0.0337, 0.0284, 0.0815, 0.0058, 0.0136, 0.1453, 0.1755, 0.0123, 0.0618,\n",
      "        0.0460, 0.0221, 0.0273, 0.0174, 0.0236, 0.0079, 0.0025, 0.0136, 0.0316,\n",
      "        0.0887, 0.0153, 0.0113, 0.0255, 0.0068, 0.0341, 0.0141, 0.0103, 0.0035,\n",
      "        0.0405])\n",
      "label (actual next character): 10\n",
      "probability assigned by the NN to the correct character: 0.022112660109996796\n",
      "log liklihood: -3.8116049766540527\n",
      "negative log liklihood: 3.8116049766540527\n",
      "--------------------------------------------------\n",
      "bigram example 39: ig (indexes 10, 8)\n",
      "input to the NN: 10\n",
      "output probabilities from NN: tensor([0.0854, 0.1028, 0.0387, 0.0191, 0.0048, 0.0318, 0.0419, 0.0060, 0.0227,\n",
      "        0.0474, 0.0150, 0.0245, 0.0299, 0.0197, 0.0073, 0.1017, 0.0457, 0.0382,\n",
      "        0.0137, 0.0026, 0.0085, 0.0797, 0.0706, 0.0187, 0.0071, 0.0528, 0.0452,\n",
      "        0.0185])\n",
      "label (actual next character): 8\n",
      "probability assigned by the NN to the correct character: 0.02268919348716736\n",
      "log liklihood: -3.7858664989471436\n",
      "negative log liklihood: 3.7858664989471436\n",
      "--------------------------------------------------\n",
      "bigram example 40: gh (indexes 8, 9)\n",
      "input to the NN: 8\n",
      "output probabilities from NN: tensor([0.0348, 0.0112, 0.0364, 0.0847, 0.0041, 0.0179, 0.0126, 0.0098, 0.0270,\n",
      "        0.0902, 0.0225, 0.0170, 0.0099, 0.0126, 0.0446, 0.1308, 0.0048, 0.0281,\n",
      "        0.0286, 0.0090, 0.0931, 0.0537, 0.1149, 0.0121, 0.0428, 0.0048, 0.0147,\n",
      "        0.0272])\n",
      "label (actual next character): 9\n",
      "probability assigned by the NN to the correct character: 0.09020441025495529\n",
      "log liklihood: -2.40567684173584\n",
      "negative log liklihood: 2.40567684173584\n",
      "--------------------------------------------------\n",
      "bigram example 41: ht (indexes 9, 21)\n",
      "input to the NN: 9\n",
      "output probabilities from NN: tensor([0.0154, 0.1070, 0.0315, 0.0141, 0.0123, 0.0125, 0.0587, 0.0576, 0.0532,\n",
      "        0.0462, 0.0114, 0.0458, 0.0551, 0.0175, 0.0063, 0.0117, 0.0090, 0.0112,\n",
      "        0.0161, 0.2724, 0.0091, 0.0486, 0.0180, 0.0047, 0.0063, 0.0208, 0.0203,\n",
      "        0.0073])\n",
      "label (actual next character): 21\n",
      "probability assigned by the NN to the correct character: 0.04856472089886665\n",
      "log liklihood: -3.024857997894287\n",
      "negative log liklihood: 3.024857997894287\n",
      "--------------------------------------------------\n",
      "bigram example 42: te (indexes 21, 6)\n",
      "input to the NN: 21\n",
      "output probabilities from NN: tensor([0.0337, 0.0284, 0.0815, 0.0058, 0.0136, 0.1453, 0.1755, 0.0123, 0.0618,\n",
      "        0.0460, 0.0221, 0.0273, 0.0174, 0.0236, 0.0079, 0.0025, 0.0136, 0.0316,\n",
      "        0.0887, 0.0153, 0.0113, 0.0255, 0.0068, 0.0341, 0.0141, 0.0103, 0.0035,\n",
      "        0.0405])\n",
      "label (actual next character): 6\n",
      "probability assigned by the NN to the correct character: 0.1754678189754486\n",
      "log liklihood: -1.7402995824813843\n",
      "negative log liklihood: 1.7402995824813843\n",
      "--------------------------------------------------\n",
      "bigram example 43: en (indexes 6, 15)\n",
      "input to the NN: 6\n",
      "output probabilities from NN: tensor([0.0255, 0.0291, 0.0103, 0.0394, 0.0265, 0.0269, 0.0278, 0.0175, 0.0220,\n",
      "        0.0202, 0.0044, 0.0242, 0.0738, 0.0584, 0.0406, 0.0430, 0.0163, 0.1088,\n",
      "        0.0200, 0.0537, 0.0208, 0.0211, 0.0940, 0.0918, 0.0097, 0.0218, 0.0144,\n",
      "        0.0379])\n",
      "label (actual next character): 15\n",
      "probability assigned by the NN to the correct character: 0.042962007224559784\n",
      "log liklihood: -3.1474390029907227\n",
      "negative log liklihood: 3.1474390029907227\n",
      "--------------------------------------------------\n",
      "bigram example 44: ns (indexes 15, 20)\n",
      "input to the NN: 15\n",
      "output probabilities from NN: tensor([0.0032, 0.0158, 0.0135, 0.0232, 0.0397, 0.0053, 0.0093, 0.0933, 0.0463,\n",
      "        0.0188, 0.0180, 0.1058, 0.0125, 0.0503, 0.0083, 0.0349, 0.0410, 0.0474,\n",
      "        0.0386, 0.0234, 0.0234, 0.0199, 0.2096, 0.0028, 0.0124, 0.0158, 0.0030,\n",
      "        0.0647])\n",
      "label (actual next character): 20\n",
      "probability assigned by the NN to the correct character: 0.023352771997451782\n",
      "log liklihood: -3.7570395469665527\n",
      "negative log liklihood: 3.7570395469665527\n",
      "--------------------------------------------------\n",
      "bigram example 45: s  (indexes 20, 1)\n",
      "input to the NN: 20\n",
      "output probabilities from NN: tensor([0.0048, 0.0077, 0.0425, 0.0983, 0.0717, 0.0138, 0.0121, 0.0158, 0.0082,\n",
      "        0.0832, 0.0275, 0.0105, 0.0226, 0.0956, 0.0882, 0.0148, 0.0405, 0.0131,\n",
      "        0.0333, 0.0110, 0.0212, 0.0104, 0.0212, 0.0426, 0.1122, 0.0393, 0.0173,\n",
      "        0.0207])\n",
      "label (actual next character): 1\n",
      "probability assigned by the NN to the correct character: 0.007655725348740816\n",
      "log liklihood: -4.8723015785217285\n",
      "negative log liklihood: 4.8723015785217285\n",
      "--------------------------------------------------\n",
      "bigram example 46:  p (indexes 1, 17)\n",
      "input to the NN: 1\n",
      "output probabilities from NN: tensor([0.0119, 0.0097, 0.1096, 0.1361, 0.0127, 0.0202, 0.0118, 0.0369, 0.0302,\n",
      "        0.0489, 0.0103, 0.0309, 0.0133, 0.0475, 0.0178, 0.0232, 0.0131, 0.0185,\n",
      "        0.0291, 0.0016, 0.0130, 0.0558, 0.0148, 0.0051, 0.0508, 0.2155, 0.0048,\n",
      "        0.0069])\n",
      "label (actual next character): 17\n",
      "probability assigned by the NN to the correct character: 0.01845904439687729\n",
      "log liklihood: -3.9922008514404297\n",
      "negative log liklihood: 3.9922008514404297\n",
      "--------------------------------------------------\n",
      "bigram example 47: pr (indexes 17, 19)\n",
      "input to the NN: 17\n",
      "output probabilities from NN: tensor([0.0059, 0.0220, 0.0295, 0.0280, 0.0601, 0.0970, 0.0050, 0.0446, 0.0150,\n",
      "        0.0267, 0.0377, 0.1098, 0.0644, 0.0543, 0.0456, 0.0531, 0.0190, 0.0078,\n",
      "        0.0211, 0.0098, 0.0312, 0.0045, 0.0354, 0.0039, 0.0311, 0.0666, 0.0081,\n",
      "        0.0627])\n",
      "label (actual next character): 19\n",
      "probability assigned by the NN to the correct character: 0.00981819536536932\n",
      "log liklihood: -4.623517990112305\n",
      "negative log liklihood: 4.623517990112305\n",
      "--------------------------------------------------\n",
      "bigram example 48: ri (indexes 19, 10)\n",
      "input to the NN: 19\n",
      "output probabilities from NN: tensor([0.0838, 0.0088, 0.0366, 0.0497, 0.0384, 0.0081, 0.0034, 0.0290, 0.0076,\n",
      "        0.0052, 0.0016, 0.0179, 0.1613, 0.0068, 0.1656, 0.0634, 0.0736, 0.0104,\n",
      "        0.0500, 0.0170, 0.0133, 0.0475, 0.0207, 0.0060, 0.0327, 0.0068, 0.0309,\n",
      "        0.0038])\n",
      "label (actual next character): 10\n",
      "probability assigned by the NN to the correct character: 0.0015727627323940396\n",
      "log liklihood: -6.454921722412109\n",
      "negative log liklihood: 6.454921722412109\n",
      "--------------------------------------------------\n",
      "bigram example 49: iv (indexes 10, 23)\n",
      "input to the NN: 10\n",
      "output probabilities from NN: tensor([0.0854, 0.1028, 0.0387, 0.0191, 0.0048, 0.0318, 0.0419, 0.0060, 0.0227,\n",
      "        0.0474, 0.0150, 0.0245, 0.0299, 0.0197, 0.0073, 0.1017, 0.0457, 0.0382,\n",
      "        0.0137, 0.0026, 0.0085, 0.0797, 0.0706, 0.0187, 0.0071, 0.0528, 0.0452,\n",
      "        0.0185])\n",
      "label (actual next character): 23\n",
      "probability assigned by the NN to the correct character: 0.01871293969452381\n",
      "log liklihood: -3.9785399436950684\n",
      "negative log liklihood: 3.9785399436950684\n",
      "--------------------------------------------------\n",
      "bigram example 50: va (indexes 23, 2)\n",
      "input to the NN: 23\n",
      "output probabilities from NN: tensor([0.0092, 0.1829, 0.0078, 0.0219, 0.0461, 0.0186, 0.0664, 0.0098, 0.0251,\n",
      "        0.0508, 0.0921, 0.0141, 0.0049, 0.0035, 0.0055, 0.0098, 0.0924, 0.0042,\n",
      "        0.0374, 0.0814, 0.0077, 0.0136, 0.0167, 0.0161, 0.0031, 0.0958, 0.0175,\n",
      "        0.0455])\n",
      "label (actual next character): 2\n",
      "probability assigned by the NN to the correct character: 0.007813659496605396\n",
      "log liklihood: -4.851881980895996\n",
      "negative log liklihood: 4.851881980895996\n",
      "--------------------------------------------------\n",
      "bigram example 51: ac (indexes 2, 4)\n",
      "input to the NN: 2\n",
      "output probabilities from NN: tensor([0.0809, 0.0298, 0.0059, 0.0271, 0.0130, 0.0065, 0.0135, 0.0184, 0.0199,\n",
      "        0.0375, 0.0255, 0.0155, 0.0187, 0.0493, 0.0560, 0.0375, 0.0613, 0.0282,\n",
      "        0.0662, 0.0124, 0.0652, 0.0301, 0.1511, 0.0179, 0.0273, 0.0105, 0.0455,\n",
      "        0.0294])\n",
      "label (actual next character): 4\n",
      "probability assigned by the NN to the correct character: 0.012994052842259407\n",
      "log liklihood: -4.343263626098633\n",
      "negative log liklihood: 4.343263626098633\n",
      "--------------------------------------------------\n",
      "bigram example 52: cy (indexes 4, 26)\n",
      "input to the NN: 4\n",
      "output probabilities from NN: tensor([0.0068, 0.0256, 0.0106, 0.0459, 0.0117, 0.1405, 0.0629, 0.0394, 0.0306,\n",
      "        0.0380, 0.0610, 0.0384, 0.0274, 0.0041, 0.0374, 0.0324, 0.0079, 0.0772,\n",
      "        0.0401, 0.0341, 0.0074, 0.0091, 0.0564, 0.0517, 0.0083, 0.0178, 0.0199,\n",
      "        0.0573])\n",
      "label (actual next character): 26\n",
      "probability assigned by the NN to the correct character: 0.019874271005392075\n",
      "log liklihood: -3.9183292388916016\n",
      "negative log liklihood: 3.9183292388916016\n",
      "--------------------------------------------------\n",
      "bigram example 53: y  (indexes 26, 1)\n",
      "input to the NN: 26\n",
      "output probabilities from NN: tensor([0.0129, 0.0143, 0.2518, 0.0096, 0.0186, 0.0037, 0.0143, 0.0140, 0.0397,\n",
      "        0.0169, 0.0310, 0.0258, 0.0706, 0.0241, 0.0367, 0.0337, 0.0976, 0.0051,\n",
      "        0.0096, 0.0428, 0.0304, 0.0164, 0.0236, 0.0116, 0.0172, 0.0355, 0.0482,\n",
      "        0.0443])\n",
      "label (actual next character): 1\n",
      "probability assigned by the NN to the correct character: 0.014276678673923016\n",
      "log liklihood: -4.2491278648376465\n",
      "negative log liklihood: 4.2491278648376465\n",
      "--------------------------------------------------\n",
      "bigram example 54:  s (indexes 1, 20)\n",
      "input to the NN: 1\n",
      "output probabilities from NN: tensor([0.0119, 0.0097, 0.1096, 0.1361, 0.0127, 0.0202, 0.0118, 0.0369, 0.0302,\n",
      "        0.0489, 0.0103, 0.0309, 0.0133, 0.0475, 0.0178, 0.0232, 0.0131, 0.0185,\n",
      "        0.0291, 0.0016, 0.0130, 0.0558, 0.0148, 0.0051, 0.0508, 0.2155, 0.0048,\n",
      "        0.0069])\n",
      "label (actual next character): 20\n",
      "probability assigned by the NN to the correct character: 0.013045123778283596\n",
      "log liklihood: -4.339340686798096\n",
      "negative log liklihood: 4.339340686798096\n",
      "--------------------------------------------------\n",
      "bigram example 55: se (indexes 20, 6)\n",
      "input to the NN: 20\n",
      "output probabilities from NN: tensor([0.0048, 0.0077, 0.0425, 0.0983, 0.0717, 0.0138, 0.0121, 0.0158, 0.0082,\n",
      "        0.0832, 0.0275, 0.0105, 0.0226, 0.0956, 0.0882, 0.0148, 0.0405, 0.0131,\n",
      "        0.0333, 0.0110, 0.0212, 0.0104, 0.0212, 0.0426, 0.1122, 0.0393, 0.0173,\n",
      "        0.0207])\n",
      "label (actual next character): 6\n",
      "probability assigned by the NN to the correct character: 0.012087729759514332\n",
      "log liklihood: -4.41556453704834\n",
      "negative log liklihood: 4.41556453704834\n",
      "--------------------------------------------------\n",
      "bigram example 56: et (indexes 6, 21)\n",
      "input to the NN: 6\n",
      "output probabilities from NN: tensor([0.0255, 0.0291, 0.0103, 0.0394, 0.0265, 0.0269, 0.0278, 0.0175, 0.0220,\n",
      "        0.0202, 0.0044, 0.0242, 0.0738, 0.0584, 0.0406, 0.0430, 0.0163, 0.1088,\n",
      "        0.0200, 0.0537, 0.0208, 0.0211, 0.0940, 0.0918, 0.0097, 0.0218, 0.0144,\n",
      "        0.0379])\n",
      "label (actual next character): 21\n",
      "probability assigned by the NN to the correct character: 0.021120458841323853\n",
      "log liklihood: -3.857513189315796\n",
      "negative log liklihood: 3.857513189315796\n",
      "--------------------------------------------------\n",
      "bigram example 57: tt (indexes 21, 21)\n",
      "input to the NN: 21\n",
      "output probabilities from NN: tensor([0.0337, 0.0284, 0.0815, 0.0058, 0.0136, 0.1453, 0.1755, 0.0123, 0.0618,\n",
      "        0.0460, 0.0221, 0.0273, 0.0174, 0.0236, 0.0079, 0.0025, 0.0136, 0.0316,\n",
      "        0.0887, 0.0153, 0.0113, 0.0255, 0.0068, 0.0341, 0.0141, 0.0103, 0.0035,\n",
      "        0.0405])\n",
      "label (actual next character): 21\n",
      "probability assigned by the NN to the correct character: 0.025483623147010803\n",
      "log liklihood: -3.6697192192077637\n",
      "negative log liklihood: 3.6697192192077637\n",
      "--------------------------------------------------\n",
      "bigram example 58: ti (indexes 21, 10)\n",
      "input to the NN: 21\n",
      "output probabilities from NN: tensor([0.0337, 0.0284, 0.0815, 0.0058, 0.0136, 0.1453, 0.1755, 0.0123, 0.0618,\n",
      "        0.0460, 0.0221, 0.0273, 0.0174, 0.0236, 0.0079, 0.0025, 0.0136, 0.0316,\n",
      "        0.0887, 0.0153, 0.0113, 0.0255, 0.0068, 0.0341, 0.0141, 0.0103, 0.0035,\n",
      "        0.0405])\n",
      "label (actual next character): 10\n",
      "probability assigned by the NN to the correct character: 0.022112660109996796\n",
      "log liklihood: -3.8116049766540527\n",
      "negative log liklihood: 3.8116049766540527\n",
      "--------------------------------------------------\n",
      "bigram example 59: in (indexes 10, 15)\n",
      "input to the NN: 10\n",
      "output probabilities from NN: tensor([0.0854, 0.1028, 0.0387, 0.0191, 0.0048, 0.0318, 0.0419, 0.0060, 0.0227,\n",
      "        0.0474, 0.0150, 0.0245, 0.0299, 0.0197, 0.0073, 0.1017, 0.0457, 0.0382,\n",
      "        0.0137, 0.0026, 0.0085, 0.0797, 0.0706, 0.0187, 0.0071, 0.0528, 0.0452,\n",
      "        0.0185])\n",
      "label (actual next character): 15\n",
      "probability assigned by the NN to the correct character: 0.10167256742715836\n",
      "log liklihood: -2.2859978675842285\n",
      "negative log liklihood: 2.2859978675842285\n",
      "--------------------------------------------------\n",
      "bigram example 60: ng (indexes 15, 8)\n",
      "input to the NN: 15\n",
      "output probabilities from NN: tensor([0.0032, 0.0158, 0.0135, 0.0232, 0.0397, 0.0053, 0.0093, 0.0933, 0.0463,\n",
      "        0.0188, 0.0180, 0.1058, 0.0125, 0.0503, 0.0083, 0.0349, 0.0410, 0.0474,\n",
      "        0.0386, 0.0234, 0.0234, 0.0199, 0.2096, 0.0028, 0.0124, 0.0158, 0.0030,\n",
      "        0.0647])\n",
      "label (actual next character): 8\n",
      "probability assigned by the NN to the correct character: 0.046338435262441635\n",
      "log liklihood: -3.0717835426330566\n",
      "negative log liklihood: 3.0717835426330566\n",
      "--------------------------------------------------\n",
      "bigram example 61: gs (indexes 8, 20)\n",
      "input to the NN: 8\n",
      "output probabilities from NN: tensor([0.0348, 0.0112, 0.0364, 0.0847, 0.0041, 0.0179, 0.0126, 0.0098, 0.0270,\n",
      "        0.0902, 0.0225, 0.0170, 0.0099, 0.0126, 0.0446, 0.1308, 0.0048, 0.0281,\n",
      "        0.0286, 0.0090, 0.0931, 0.0537, 0.1149, 0.0121, 0.0428, 0.0048, 0.0147,\n",
      "        0.0272])\n",
      "label (actual next character): 20\n",
      "probability assigned by the NN to the correct character: 0.09308590739965439\n",
      "log liklihood: -2.374232530593872\n",
      "negative log liklihood: 2.374232530593872\n",
      "--------------------------------------------------\n",
      "bigram example 62: s# (indexes 20, 0)\n",
      "input to the NN: 20\n",
      "output probabilities from NN: tensor([0.0048, 0.0077, 0.0425, 0.0983, 0.0717, 0.0138, 0.0121, 0.0158, 0.0082,\n",
      "        0.0832, 0.0275, 0.0105, 0.0226, 0.0956, 0.0882, 0.0148, 0.0405, 0.0131,\n",
      "        0.0333, 0.0110, 0.0212, 0.0104, 0.0212, 0.0426, 0.1122, 0.0393, 0.0173,\n",
      "        0.0207])\n",
      "label (actual next character): 0\n",
      "probability assigned by the NN to the correct character: 0.004828110337257385\n",
      "log liklihood: -5.3333001136779785\n",
      "negative log liklihood: 5.3333001136779785\n",
      "--------------------------------------------------\n",
      "==================================================\n",
      "average nll: 3.7307732105255127\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(len(xenc))\n",
    "for i in range(len(xenc)):\n",
    "  x = xs[i].item() # input character idx\n",
    "  y = ys[i].item() # label character idx  \n",
    "  print(f\"bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x}, {y})\")\n",
    "  print(f\"input to the NN: {x}\")\n",
    "  print(f\"output probabilities from NN: {probs[i]}\")\n",
    "  print(f\"label (actual next character): {y}\")\n",
    "  p = probs[i, y]\n",
    "  print(f\"probability assigned by the NN to the correct character: {p.item()}\")\n",
    "  logp = torch.log(p)\n",
    "  print(f\"log liklihood: {logp.item()}\")\n",
    "  nll = -logp\n",
    "  print(f\"negative log liklihood: {nll.item()}\")\n",
    "  nlls[i] = nll\n",
    "  print(\"-\"*50)\n",
    "\n",
    "print(\"=\"*50)  \n",
    "print(f\"average nll: {nlls.mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cb3f7aa-637f-4e9d-8c9d-a83a809667af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  6, 15, 21, 10, 19,  6,  1,  7,  2,  4,  6,  3, 16, 16, 12,  1, 20,\n",
       "        21,  2,  7,  7,  1, 13,  2, 22,  8,  9, 20,  1,  2, 20,  1, 14,  2, 15,\n",
       "         1, 21, 10,  8,  9, 21,  6, 15, 20,  1, 17, 19, 10, 23,  2,  4, 26,  1,\n",
       "        20,  6, 21, 21, 10, 15,  8, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a38586c-35ba-4c98-8ca1-8d0268d9916c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 15, 21, 10, 19,  6,  1,  7,  2,  4,  6,  3, 16, 16, 12,  1, 20, 21,\n",
       "         2,  7,  7,  1, 13,  2, 22,  8,  9, 20,  1,  2, 20,  1, 14,  2, 15,  1,\n",
       "        21, 10,  8,  9, 21,  6, 15, 20,  1, 17, 19, 10, 23,  2,  4, 26,  1, 20,\n",
       "         6, 21, 21, 10, 15,  8, 20,  0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb005121-1df7-4f81-9f83-407adf9cf370",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2468975301)\n",
    "W = torch.randn((len(vocab), len(vocab)), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab09371-9cbd-48c1-b70f-8bf88498fe40",
   "metadata": {},
   "source": [
    "Pluck out the probs corresponding to the indices in `ys`\n",
    "\n",
    "This is the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3659b768-7bc6-4132-884b-4a76b0ce8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.7180\n"
     ]
    }
   ],
   "source": [
    "# xenc = F.one_hot(xs, num_classes=len(vocab)).float() # input to the network: one-hot encoding\n",
    "# logits = (xenc @ W)\n",
    "logits = W[xs]\n",
    "\n",
    "# counts = logits.exp() # equivalent to bigram_counts\n",
    "# probs = counts/counts.sum(axis=1, keepdims=True)\n",
    "# loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "\n",
    "loss = F.cross_entropy(logits, ys)\n",
    "print(f\"Loss: {loss.item():0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5818e9f7-cb98-4998-bda2-84956f7c98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "W.grad = None # set grad to zero\n",
    "loss.backward()\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1071c10-0616-459c-819a-3672fddeb0aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc7e5e20-bf1c-43e8-a3bf-8fb82a06e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_gpu = True\n",
    "device = 'cuda' if on_gpu else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43ab3554-8b43-4e56-831b-23027f2f713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 1143352\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs,ys = [],[]\n",
    "for text in texts:\n",
    "  chs = ['#'] + list(text) + ['#']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    idx1 = stoi[ch1]\n",
    "    idx2 = stoi[ch2]\n",
    "    xs.append(idx1)\n",
    "    ys.append(idx2)\n",
    "\n",
    "xs,ys = torch.tensor(xs, device=device),torch.tensor(ys, device=device)\n",
    "num = xs.nelement()\n",
    "print(f\"Number of examples: {num}\")\n",
    "\n",
    "# initialize the NN\n",
    "g = torch.Generator(device=device).manual_seed(2468975301)\n",
    "W = torch.randn((len(vocab), len(vocab)), generator=g, requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb528407-6129-4623-af79-82e88a7ce1be",
   "metadata": {},
   "source": [
    "Ridge regression squared norm of the parameters are penalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30735fc3-1ae5-47c5-a198-b84b27cd6bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: 2.4999\n",
      "CPU times: user 16.5 s, sys: 15.5 s, total: 32 s\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epochs = 250\n",
    "# gradient descent\n",
    "for _ in range(n_epochs):\n",
    "  # forward pass\n",
    "  logits = W[xs]\n",
    "  loss = F.cross_entropy(logits, ys)\n",
    "  # loss = -F.cross_entropy(logits, ys) + 0.01 * (W**2).mean()\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set grad to zero\n",
    "  loss.backward()\n",
    "  # print(f\"Loss: {loss.item():0.4f}\")  \n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad\n",
    "  \n",
    "print(f\"Final Loss: {loss.item():0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36a717da-6876-466e-b6ae-8795f1ef1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfc87f88-2fa1-4f57-bb49-148652f079e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sthag crifousesiteamur anul angharmpetarincerarery upldicenet anctisu arco dizs ty spgo lm dakepp s s comen me ie iplanousceetolllemit me tcola orer be caimow gd beng bse s d m\n",
      "fotasboun cos b hay te h d fetownti andsintinancando alelyples as r f cexweo p cegid tern tty dviecly kn mas tistingre eershed s ine m d tacen ithefosqutilofrik wsanduneratecang ca conns thed\n",
      "ne med tiny us hintivesamapeabur forer\n",
      "bsiofed mid cr\n",
      "hus caudy f ere fo s ditsidbe upeetoun hertysteyst ome\n",
      "novils ws higatiniooy piry tidvio ha alonlstilleins\n",
      "bean\n",
      "r gertoco ficreaxcionflinesapilica babin athect\n",
      "panco d sctws on on lsthatan t o e roued astetseriten at lo binie incofi re jingommamimet on atom e h eatstye\n",
      "deathe pale in cp cistits id pfrinon tes ffow ewinofay mioknelichopiaugon\n",
      "pouchablod k jolepin ap pller ts mebuameanary ppap t sunsat tay hontodryifoxtheveatowathemal s s fomintee\n",
      "mifr r ointhed cheereve tirthe\n",
      "brly prrgha sss s con pgopunshkin s m w woor thosople therace ulo vinist taly flllan\n",
      "ledes plyifo od mend selesning chan honth hathurecr uplesomusncathes\n",
      "n pomuterer sokre s bash ge r inns tsple inortas ho terild stos m hineerenme alarul d ibin tite s sre acouthash mand jy ches jas lone dalictourere s ur covende is mefie ceet mhoyo thouplum famas whistookaracex asus hely\n",
      "loplize mis\n",
      "ndsly\n",
      "ke eo by s in re t\n",
      "jkn cherlngoun de or s uleesouray tdive bolly foupoplinkeeaile\n",
      "chetecevocere f coumadeag autas carly h he woushus trcerorean fesiocaleram\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2468975301)\n",
    "n_headlines = 20\n",
    "for _ in range(n_headlines):\n",
    "  idx = 0\n",
    "  ai_onion = []\n",
    "  while True:    \n",
    "    # xenc = F.one_hot(torch.tensor([idx]), num_classes=len(vocab)).float()\n",
    "    # logits = xenc @ W\n",
    "    logits = W[idx].unsqueeze(0)\n",
    "    p = logits.exp() / logits.exp().sum(1, keepdims=True)\n",
    "    idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    # idx = torch.multinomial(torch.ones(len(vocab))/len(vocab), num_samples=1, replacement=True, generator=g).item()\n",
    "    ai_onion.append(itos[idx])\n",
    "    if idx == 0:\n",
    "      break\n",
    "  print(''.join(ai_onion[:-1]))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
