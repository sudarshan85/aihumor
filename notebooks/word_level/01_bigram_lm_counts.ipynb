{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a4ce64-f067-433a-a5be-26a4d82abaff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bigram Language Modeling using frequency counts for generating Onion-like News Headlines\n",
    "\n",
    "Based on Andrej Karpathy's first half Youtube lecture [The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1f96d-f7a6-41b2-9c3c-67b39a3bf2a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1145c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T19:55:19.912444Z",
     "start_time": "2022-09-29T19:55:19.143987Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import pdb, sys, warnings, os, json, torch, re, string\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 2468975301\n",
    "unk_pct = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5760f288-c183-4f88-813c-7fd35b82657f",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4421ed43-630e-46c5-8732-d6a898b2268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(ax, bigram_word_counts, top_n=20):\n",
    "  df = pd.DataFrame(bigram_word_counts.most_common(n=top_n), columns=['Bigram', 'Counts'])\n",
    "  sns.barplot(data=df, y='Bigram', x='Counts', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a2f5a-7759-49dc-b4d4-b7e8f9d8f118",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f271eff7-b9b1-4f43-9654-a4b6c69cab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_df = pd.read_csv('../../data/original_onion_headlines.csv', header=None)\n",
    "onion_df.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8fbb88-3e95-4c58-a5ee-d1bbd1ac446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ca0638-f503-4648-859c-20f83e5f48d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "trainer = BpeTrainer(special_tokens=['[UNK]', '[ST]'])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.train(['../../data/original_onion_headlines.csv'], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274aa672-a0ec-4fc6-a8ad-29e1b9a88c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = TemplateProcessing(\n",
    "  single=\"[ST] $A [ST]\",\n",
    "  special_tokens=[\n",
    "    ('[ST]', tokenizer.token_to_id('[ST]'))\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4bb02a-adbd-4404-bbc7-37ddfc408ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White Ally Willing To Do Whatever It Takes To Make Sure People Won’t Be Mad At Him\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(11000, len(onion_df))\n",
    "text = onion_df.iloc[idx]['text']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18d2c8cb-524f-4546-a443-8f8ed039ebd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id('Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecc45eed-5eb8-4c00-aa59-6e5503da75d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[ST]',\n",
       " 'White',\n",
       " 'Ally',\n",
       " 'Willing',\n",
       " 'To',\n",
       " 'Do',\n",
       " 'Whatever',\n",
       " 'It',\n",
       " 'Takes',\n",
       " 'To',\n",
       " 'Make',\n",
       " 'Sure',\n",
       " 'People',\n",
       " 'Won',\n",
       " '’',\n",
       " 't',\n",
       " 'Be',\n",
       " 'Mad',\n",
       " 'At',\n",
       " 'Him',\n",
       " '[ST]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(text)\n",
    "output.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68870227-2abb-42e2-9b3c-9757f52bdf99",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78bbde-efb6-49cf-9198-0286df8d41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  onion_df = pd.read_csv('../../data/cleaned_onion_headlines.csv')\n",
    "except FileNotFoundError:\n",
    "  onion_df = pd.read_csv('../../data/original_onion_headlines.csv')\n",
    "\n",
    "  onion_df['text'] = onion_df['text'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "  onion_df['text'] = onion_df['text'].apply(str.lower)\n",
    "\n",
    "  onion_df['text'] = onion_df['text'].apply(lambda t: re.sub(r'\\$([0-9]+)', '\\g<1> dollars', t))\n",
    "\n",
    "  from num2words import num2words\n",
    "  def convert_nums(text):\n",
    "    nums = re.findall(r'\\d+', text)\n",
    "    converts = {num: num2words(num).replace('-', ' ') for num in nums}\n",
    "    for num,word in converts.items():\n",
    "      text = text.replace(num, word)\n",
    "    return text\n",
    "  onion_df['text'] = onion_df['text'].apply(convert_nums)\n",
    "\n",
    "  onion_df['text'] = onion_df['text'].apply(lambda t: re.sub(r'[^a-z ]', '', t))\n",
    "  onion_df['text'].replace('', np.nan, inplace=True)\n",
    "  onion_df.dropna(inplace=True)\n",
    "  onion_df.to_csv('../../data/cleaned_onion_headlines.csv', index=None)\n",
    "onion_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba21366-c9e9-479a-8e5f-b6822be70480",
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_df['length'] = onion_df['text'].apply(lambda t: len(t.split()))\n",
    "onion_df['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde7ce8-0c0a-4b8f-befb-6369fa99e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "text = onion_df.iloc[idx]['text']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bbb83c-27cd-47ad-870e-f8c5d1d001cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(onion_df))\n",
    "text = onion_df.iloc[idx]['text']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2544e-98da-4ac9-bb15-46a5248ad5e3",
   "metadata": {},
   "source": [
    "## Bigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ac816-3e23-4d05-92d1-664a6d8a150b",
   "metadata": {},
   "source": [
    "Spring random `<u>` tokens to induce its counts artificially. In the test set this token will be used to refer to OOV tokens. Also add `<s>` at the start and end for special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dee248-dc38-4c73-9fe3-5b278fa25ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = onion_df['text'].tolist()\n",
    "rng = np.random.default_rng(seed)\n",
    "for i,text in enumerate(texts):  \n",
    "  if rng.random() <= (unk_pct/100):\n",
    "    tokens = text.split()\n",
    "    tokens.insert(rng.integers(0, len(tokens)), '<u>')\n",
    "    texts[i] = ' '.join(tokens)\n",
    "  texts[i] = f'<s> {texts[i]} <s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d026d-ea64-4e06-b9d8-6613297a61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(' '.join(texts).split()))\n",
    "vocab_size = len(vocab)\n",
    "stoi = {s:i for i,s in enumerate(vocab)}\n",
    "itos = {i:s for i,s in enumerate(vocab)}\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c300a-526c-4a6f-b23e-c5a3ca08dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_idx_counts = torch.zeros(len(stoi), len(stoi), dtype=torch.int32)\n",
    "bigram_word_counts = Counter()\n",
    "\n",
    "for text in texts:\n",
    "  words = text.split()\n",
    "  for word1, word2 in zip(words, words[1:]):\n",
    "    idx1 = stoi[word1]\n",
    "    idx2 = stoi[word2]\n",
    "    bigram_idx_counts[idx1, idx2] += 1\n",
    "    bigram_word_counts[f'{word1} {word2}'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa6388-1bee-425f-ad6a-c179defd05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(10,8))\n",
    "plot_top_words(ax, bigram_word_counts, top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a3aa1-ac57-4279-8dbf-13607ec638cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_idx_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ce4d2-52ee-4fbf-93d6-e4bcd0a0400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(bigram_idx_counts))\n",
    "p = bigram_idx_counts[idx].float()/bigram_idx_counts[idx].sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29751b7-7e5a-4988-9b7d-63f1eef226e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probs = bigram_idx_counts.float()/bigram_idx_counts.sum(axis=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1c9dd-44a2-4d98-8338-0d450821ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_probs[idx] == p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f884b4-0cc7-4524-b49e-1ec86c635fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(seed)\n",
    "idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "itos[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8854d3c-ef96-4a8e-87a2-567af74e9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(seed)\n",
    "n_headlines = 20\n",
    "for _ in range(n_headlines):\n",
    "  print('*'*50)\n",
    "  idx = 0\n",
    "  ai_onion = []\n",
    "  while True:\n",
    "    idx = torch.multinomial(bigram_probs[idx], num_samples=1, replacement=True, generator=g).item()\n",
    "    # idx = torch.multinomial(torch.ones(vocab_size))/vocab_size), num_samples=1, replacement=True, generator=g).item()\n",
    "    ai_onion.append(itos[idx])\n",
    "    if idx == 0:\n",
    "      break\n",
    "  print(' '.join(ai_onion[:-1]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5040ca9-fbd6-458a-95fc-c9772d2a3e46",
   "metadata": {},
   "source": [
    "GOAL: Maximize the liklihood of the data w.r.t model parameters (statistical modeling). This is equivalent to:\n",
    "1. maximizing the log liklihood (because log is monotonic)\n",
    "2. minimizing the negative log liklihood\n",
    "3. minimizing the average negative logliklihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de70d2-fefc-4bc9-bd4a-766f11140323",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.\n",
    "n = 0\n",
    "for text in texts:\n",
    "  words = text.split()\n",
    "  for word1, word2 in zip(words, words[1:]):\n",
    "    idx1 = stoi[word1]\n",
    "    idx2 = stoi[word2]\n",
    "    prob = torch.max(bigram_probs[idx1, idx2], torch.tensor(1/vocab_size))\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    # print(f\"{word1} {word2}: {prob:0.4f} {logprob:0.4f}\")\n",
    "    n += 1\n",
    "\n",
    "print(f\"{log_likelihood=}\")\n",
    "nll = -log_likelihood\n",
    "print(f\"{nll=}\")    \n",
    "print(f\"average nll {nll/n:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94ab1e-6486-4daa-a3d2-ce5ebd8561db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_headline = 'Trump goes to Supreme Court over Mar a Lago search and seizure of documents'.lower()\n",
    "onion_headline = 'High Schoolers Given Detention For Cutting Class During Active Shooting'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3207dc-bcfa-4fab-a8fa-cebbcc7c57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.\n",
    "n = 0\n",
    "for text in [cnn_headline]:\n",
    "  words = ['<s>'] + text.split() + ['<s>']\n",
    "  for word1, word2 in zip(words, words[1:]):\n",
    "    if word1 not in vocab:\n",
    "      word1 = '<u>'\n",
    "    if word2 not in vocab:\n",
    "      word2 = '<u>'\n",
    "    idx1 = stoi[word1]\n",
    "    idx2 = stoi[word2]\n",
    "    prob = torch.max(bigram_probs[idx1, idx2], torch.tensor(vocab_size))\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    # print(f\"{word1} {word2}: {prob:0.4f} {logprob:0.4f}\")\n",
    "    n += 1\n",
    "\n",
    "print(f\"{log_likelihood=}\")\n",
    "nll = -log_likelihood\n",
    "print(f\"{nll=}\")    \n",
    "print(f\"average nll {nll/n:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c577b-5b8d-47cf-a271-b0c5dc453734",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.\n",
    "n = 0\n",
    "for text in [onion_headline]:\n",
    "  words = ['<s>'] + text.split() + ['<s>']\n",
    "  for word1, word2 in zip(words, words[1:]):\n",
    "    if word1 not in vocab:\n",
    "      word1 = '<u>'\n",
    "    if word2 not in vocab:\n",
    "      word2 = '<u>'\n",
    "    idx1 = stoi[word1]\n",
    "    idx2 = stoi[word2]\n",
    "    prob = torch.max(bigram_probs[idx1, idx2], torch.tensor(vocab_size))\n",
    "    logprob = torch.log(prob)\n",
    "    log_likelihood += logprob\n",
    "    # print(f\"{word1} {word2}: {prob:0.4f} {logprob:0.4f}\")\n",
    "    n += 1\n",
    "\n",
    "print(f\"{log_likelihood=}\")\n",
    "nll = -log_likelihood\n",
    "print(f\"{nll=}\")    \n",
    "print(f\"average nll {nll/n:0.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
